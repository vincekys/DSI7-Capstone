{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Reviews from Agoda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from time import sleep, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create empty lists to store scrapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_body = []\n",
    "review_title = []\n",
    "trip_type = [] \n",
    "review_date = []\n",
    "stay_date = []\n",
    "nationality = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create url variable which is the first page of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.agoda.com/en-sg/mandarin-orchard-singapore/hotel/singapore-sg.html?cid=-218'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create function which does the scraping. It takes the url given and the maximum number of pages to scrape as arguments and populates scrapped data into the empty lists created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper_agoda(url, max_pages):\n",
    "    \n",
    "    start_time = time()\n",
    "    \n",
    "    page = 1\n",
    "    \n",
    "    \n",
    "    # OPENS BROWSER\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # OPENS FIRST PAGE\n",
    "    driver.get(url)\n",
    "    \n",
    "    sleep(5)\n",
    "    \n",
    "    # before scraping starts, we need to close the window that prompts the input of dates\n",
    "    close = driver.find_elements_by_xpath(\"//a[@class='AlertMessage__close CalendarAlertMessage__close']\")\n",
    "    close[0].click()\n",
    "    \n",
    "    # ADJUST THE MAX NUMBER OF PAGES TO SCRAPE\n",
    "    while page <= max_pages:\n",
    "\n",
    "\n",
    "        # GET INNER HTML\n",
    "        soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "\n",
    "        # CONTAINER IN PAGE\n",
    "        containers = soup.findAll('div', {'class': 'Review-comment'})\n",
    "\n",
    "\n",
    "\n",
    "        # LOOPING THROUGH EACH OF THE CONTAINERS (each represents ONE user's review)\n",
    "        for container in containers:\n",
    "\n",
    "\n",
    "            # STAY DATE (if empty append None, else append get_text())\n",
    "            stay_date_item = container.findAll('span')[2]\n",
    "            if stay_date_item != None:\n",
    "                stay_date.append(stay_date_item.get_text())\n",
    "            else:\n",
    "                stay_date.append(None)\n",
    "\n",
    "\n",
    "\n",
    "            # TRIP TYPE (if empty append None, else append get_text())\n",
    "            trip_type_item = container.find('span')\n",
    "            if trip_type_item != None:\n",
    "                trip_type.append(trip_type_item.get_text())\n",
    "            else:\n",
    "                trip_type.append(None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # REVIEWER NATIONALITY (if empty append None, else append get_text())\n",
    "            nationality_item = container.find('div', {'class': 'Review-comment-reviewer'})\n",
    "            if nationality_item != None:\n",
    "                nationality.append(nationality_item.get_text())\n",
    "            else:\n",
    "                nationality.append(None)\n",
    "\n",
    "\n",
    "\n",
    "            # REVIEW TITLE (if empty append None, else append get_text())\n",
    "            review_title_item = container.find('p', {'class': 'Review-comment-bodyTitle'})\n",
    "            if review_title_item != None:\n",
    "                review_title.append(review_title_item.get_text())\n",
    "            else:\n",
    "                review_title.append(None)\n",
    "\n",
    "\n",
    "\n",
    "            # REVIEW BODY (if empty append None, else append get_text())\n",
    "            review_body_item = container.find('p', {'class': 'Review-comment-bodyText'})\n",
    "            if review_body_item != None:\n",
    "                review_body.append(review_body_item.get_text())\n",
    "            else:\n",
    "                review_body.append(None)\n",
    "\n",
    "\n",
    "\n",
    "            # REVIEW DATE (if empty append None, else append get_text())\n",
    "            review_date_item = container.find('span', {'class': 'Review-statusBar-date '})\n",
    "            if review_date_item != None:\n",
    "                review_date.append(review_date_item.get_text())\n",
    "            else:\n",
    "                review_date.append(None)\n",
    "\n",
    "\n",
    "\n",
    "        # increase page by 1\n",
    "        page = page + 1\n",
    "\n",
    "\n",
    "        # toggle to next page\n",
    "\n",
    "        next_page_button = driver.find_elements_by_xpath(\"//i[@class='ficon ficon-24 ficon-carrouselarrow-right']\")\n",
    "        next_page_button[0].click()\n",
    "        sleep(5)\n",
    "\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    total_elapsed_time = time() - start_time\n",
    "    print('Total Run Time:', total_elapsed_time/60, 'min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check scrapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rows of stay_date',len(stay_date))\n",
    "print('rows of trip_type',len(trip_type))\n",
    "print('rows of review_date',len(review_date))\n",
    "print('rows of review_body',len(review_body))\n",
    "print('rows of review_title',len(review_title))\n",
    "print('rows of nationality',len(review_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all into dataframe\n",
    "zippedList =  list(zip(stay_date, trip_type, review_date, review_body, review_title, nationality))\n",
    "\n",
    "reviews = pd.DataFrame(zippedList, columns = ['date_of_stay' , 'trip_type', 'date_of_review',\\\n",
    "                                              'review', 'review_title', 'nationality'])\n",
    "\n",
    "print(reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to csv (initial scrape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_csv('./Data/MOH_AGODA_REVIEWS.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append to csv (subsequent scrape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_csv('./Data/MOH_AGODA_REVIEWS.csv', index=False, mode='a', header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
